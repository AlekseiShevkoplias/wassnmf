{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the exact Julia-saved X\n",
    "# X = np.loadtxt(\"../data/X_data.csv\", delimiter=\",\")\n",
    "# print(\"Loaded X shape:\", X.shape)\n",
    "\n",
    "# coord = np.loadtxt(\"../data/coord_data.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# C = pairwise_distances(coord.reshape(-1, 1), metric=\"sqeuclidean\")\n",
    "# C = C / C.mean()  # same normalization as the notebook\n",
    "\n",
    "# eps = 0.25\n",
    "# K = np.exp(-C / eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Insert \"src\" into sys.path so we can import the local wassnmf package\n",
    "sys.path.insert(0, \"../src\")\n",
    "from wassnmf.wassnmf import WassersteinNMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, mu, sigma=1.0):\n",
    "    \"\"\"Mimic the Julia f(coord, μ, σ): exp.(-(x .- μ).^2).\"\"\"\n",
    "    return np.exp(-(x - mu)**2 / (2 * sigma**2))  # Gaussian bump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# n_features = 100\n",
    "# n_samples = 100\n",
    "# coord = np.linspace(-12, 12, n_features)\n",
    "# X = np.zeros((n_features, n_samples), dtype=np.float64)\n",
    "\n",
    "# # Generate data as sums of 3 random Gaussian bumps per column\n",
    "# sigma = 1.0\n",
    "# for j in range(n_samples):\n",
    "#     bump1 = np.random.rand() * f(coord, sigma * np.random.randn() + 6, sigma=1.0)\n",
    "#     bump2 = np.random.rand() * f(coord, sigma * np.random.randn(), sigma=1.0)\n",
    "#     bump3 = np.random.rand() * f(coord, sigma * np.random.randn() - 6, sigma=1.0)\n",
    "#     X[:, j] = bump1 + bump2 + bump3\n",
    "\n",
    "# # Normalize columns to sum to 1 (probability simplex)\n",
    "# X /= X.sum(axis=0, keepdims=True)\n",
    "\n",
    "# print(\"X shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build cost matrix C from the same coordinate range [-12, 12]\n",
    "# C = pairwise_distances(coord.reshape(-1, 1), metric='sqeuclidean')\n",
    "# C /= C.mean()\n",
    "\n",
    "# # C = np.ones((coord.size, coord.size))\n",
    "\n",
    "# # Convert cost matrix to kernel\n",
    "# eps = 0.025\n",
    "# K = np.exp(-C / eps)\n",
    "\n",
    "# print(\"C shape:\", C.shape, \"  K shape:\", K.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "spot_data = pd.read_csv(\"../data/DLBCL_spotdata.csv\", index_col=0)\n",
    "X = spot_data.values\n",
    "coord = np.linspace(-12, 12, X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr = spot_data.T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance between features (rows are features)\n",
    "D_square = squareform(pdist(spot_data.values, metric='sqeuclidean'))\n",
    "\n",
    "# Normalize distance scale\n",
    "D_square /= np.mean(D_square)\n",
    "\n",
    "# Choose a smooth ε\n",
    "eps = 1.0  # tune this as needed\n",
    "\n",
    "# Compute Gibbs kernel\n",
    "K = np.exp(-D_square / eps)\n",
    "\n",
    "# Sanity check\n",
    "assert np.all(K > 0), \"K has zero entries!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(D_df, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(\"Euclidian Distance Between Spots\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(K, cmap=\"viridis\", square=True, cbar_kws={\"shrink\": 0.8},\n",
    "            # xticklabels=spot_data.index, yticklabels=spot_data.index\n",
    "            )\n",
    "plt.title('Kernel matrix for the WNMF (log)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.log(K), cmap=\"viridis\", square=True, cbar_kws={\"shrink\": 0.8},\n",
    "            # xticklabels=spot_data.index, yticklabels=spot_data.index\n",
    "            )\n",
    "plt.title('Kernel matrix for the WNMF (log)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X -= np.min(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build cost matrix C from the same coordinate range [-12, 12]\n",
    "# # C = pairwise_distances(coord.reshape(-1, 1), metric='sqeuclidean')\n",
    "# # C /= C.mean()\n",
    "\n",
    "# C = np.ones((coord.size, coord.size))\n",
    "\n",
    "# # Convert cost matrix to kernel\n",
    "# eps = 0.025\n",
    "# K = np.exp(-C / eps)\n",
    "\n",
    "# print(\"C shape:\", C.shape, \"  K shape:\", K.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and run WassersteinNMF with the same parameters as the Julia notebook\n",
    "wnmf = WassersteinNMF(\n",
    "    n_components=5,\n",
    "    epsilon=eps,\n",
    "    rho1=0.1,\n",
    "    rho2=0.1,\n",
    "    n_iter=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "D, Lambda = wnmf.fit_transform(X, K)\n",
    "print(\"D shape:\", D.shape)\n",
    "print(\"Lambda shape:\", Lambda.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(D, \"../results/D_250325.pt\")\n",
    "torch.save(Lambda, \"../results/Lambda_250325.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(X - D.numpy() @ Lambda.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from kneed import KneeLocator\n",
    "\n",
    "def find_best_n_components(X, k_range=range(2, 20), plot=True):\n",
    "    errors = []\n",
    "    for k in k_range:\n",
    "        model = NMF(n_components=k, init='nndsvda', random_state=0, max_iter=1000)\n",
    "        W = model.fit_transform(X)\n",
    "        H = model.components_\n",
    "        err = model.reconstruction_err_\n",
    "        errors.append(err)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(k_range, errors, marker='o')\n",
    "        plt.xlabel('Number of components (k)')\n",
    "        plt.ylabel('Reconstruction Error')\n",
    "        plt.title('NMF Model Selection')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "def find_elbow_k(errors, k_range):\n",
    "    kneedle = KneeLocator(k_range, errors, curve=\"convex\", direction=\"decreasing\")\n",
    "    return kneedle.knee\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = find_best_n_components(X, k_range=range(2, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(2, 10))\n",
    "elbow_k = find_elbow_k(errors, k_range)\n",
    "print(f\"Elbow at k = {elbow_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=5, init='nndsvda', random_state=0)\n",
    "W = model.fit_transform(X)  # shape: (100, 5)\n",
    "H = model.components_       # shape: (5, 50)\n",
    "\n",
    "# Reconstruction\n",
    "X_reconstructed = W @ H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [x.split('.')[0] for x in spot_data.columns]\n",
    "labels = pd.Series(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = labels.unique()\n",
    "palette = sns.color_palette(\"hls\", len(unique_labels))\n",
    "label_colors = dict(zip(unique_labels, palette))\n",
    "\n",
    "# Map labels to colors\n",
    "label_color_values = labels.map(label_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.clustermap(\n",
    "    X,\n",
    "    # row_colors=label_color_values.to_numpy(),\n",
    "    col_colors=label_color_values.to_numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=False,\n",
    "    yticklabels=spot_data.index,\n",
    "    figsize=(8, 4),\n",
    "    row_cluster=False,   \n",
    "    col_cluster=False\n",
    ")\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "# Add legend\n",
    "handles = [Patch(color=color, label=label) for label, color in label_colors.items()]\n",
    "plt.legend(\n",
    "    handles=handles,\n",
    "    title=\"Labels\",\n",
    "    bbox_to_anchor=(1.2, 1),\n",
    "    bbox_transform=plt.gcf().transFigure\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_matrix = torch.mm(D, Lambda)\n",
    "created_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.clustermap(\n",
    "    created_matrix,\n",
    "    # row_colors=label_color_values.to_numpy(),\n",
    "    col_colors=label_color_values.to_numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=False,\n",
    "    yticklabels=spot_data.index,\n",
    "    figsize=(8, 4),\n",
    "    row_cluster=False,   \n",
    "    col_cluster=False\n",
    ")\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "\n",
    "# Add legend\n",
    "handles = [Patch(color=color, label=label) for label, color in label_colors.items()]\n",
    "plt.legend(\n",
    "    handles=handles,\n",
    "    title=\"Labels\",\n",
    "    bbox_to_anchor=(1.2, 1),\n",
    "    bbox_transform=plt.gcf().transFigure\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_sign_names = [f'SPOT_WGC_{i}' for i in range(1, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.clustermap(\n",
    "    D,\n",
    "    # row_colors=label_color_values.to_numpy(),\n",
    "    # col_colors=label_color_values.to_numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=spot_sign_names,\n",
    "    yticklabels=spot_data.index,\n",
    "    figsize=(8, 4),\n",
    "    row_cluster=False,\n",
    "    col_cluster=False\n",
    ")\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.clustermap(\n",
    "    Lambda,\n",
    "    # row_colors=label_color_values.to_numpy(),\n",
    "    col_colors=label_color_values.to_numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=False,\n",
    "    yticklabels=spot_sign_names,\n",
    "    figsize=(8, 4),\n",
    "    row_cluster=False,\n",
    "    col_cluster=False\n",
    ")\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "g = sns.clustermap(\n",
    "    Lambda,\n",
    "    # row_colors=label_color_values.to_numpy(),\n",
    "    col_colors=label_color_values.to_numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=False,\n",
    "    yticklabels=spot_sign_names,\n",
    "    figsize=(8, 4),\n",
    "    row_cluster=False,\n",
    "    col_cluster=True,\n",
    "    # try other methods\n",
    "    method=\"ward\"\n",
    ")\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda shape: features x samples\n",
    "Lambda_df = pd.DataFrame(Lambda)  # optional, for indexing convenience\n",
    "Lambda_df.columns = labels  # or another label-aligned list\n",
    "\n",
    "# Transpose to samples x features\n",
    "Lambda_samples = Lambda_df.T\n",
    "\n",
    "# Group by label, compute centroids\n",
    "centroids = Lambda_samples.groupby(Lambda_samples.index).mean()\n",
    "\n",
    "# Compute distance matrix between centroids\n",
    "distance_matrix = pd.DataFrame(\n",
    "    squareform(pdist(centroids, metric=\"euclidean\")),\n",
    "    index=centroids.index,\n",
    "    columns=centroids.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "g = sns.clustermap(\n",
    "    distance_matrix,\n",
    "    row_colors=distance_matrix.columns.map(label_colors),\n",
    "    col_colors=distance_matrix.columns.map(label_colors),\n",
    "    cmap=\"viridis\",\n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    figsize=(8, 8),\n",
    "    row_cluster=True,\n",
    "    col_cluster=True,\n",
    "    # try other methods\n",
    "    method=\"ward\",\n",
    ")\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), rotation=0);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# Transpose Lambda: samples x features\n",
    "X = Lambda.T\n",
    "labels = pd.Series(true_labels, name=\"label\")\n",
    "\n",
    "# Fit KNN\n",
    "k = 20\n",
    "knn = NearestNeighbors(n_neighbors=k + 1).fit(X)\n",
    "distances, indices = knn.kneighbors(X)\n",
    "\n",
    "# For each sample, get label of its neighbors\n",
    "rows = []\n",
    "for i, neigh in enumerate(indices):\n",
    "    source_label = labels.iloc[i]\n",
    "    for j in neigh[1:]:  # skip self\n",
    "        target_label = labels.iloc[j]\n",
    "        rows.append((source_label, target_label))\n",
    "\n",
    "transitions = pd.DataFrame(rows, columns=[\"from\", \"to\"])\n",
    "\n",
    "# Count transitions and normalize\n",
    "transition_matrix = transitions.value_counts().unstack(fill_value=0)\n",
    "transition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0)  # row-normalize\n",
    "\n",
    "# Visualize\n",
    "sns.heatmap(transition_matrix, cmap=\"magma\", annot=True, fmt=\".2f\")\n",
    "plt.title(f\"Label-to-label transition likelihoods (k={k})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts = transitions.value_counts().unstack(fill_value=0)\n",
    "transition_probs = transition_counts.div(transition_counts.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "samples_2d = pd.DataFrame(X_2d, columns=[\"x\", \"y\"])\n",
    "samples_2d[\"label\"] = labels.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_2d = samples_2d.groupby(\"label\")[[\"x\", \"y\"]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot the samples\n",
    "sns.scatterplot(\n",
    "    data=samples_2d,\n",
    "    x=\"x\", y=\"y\",\n",
    "    hue=\"label\",\n",
    "    alpha=0.5,\n",
    "    s=100,\n",
    "    ax=ax,\n",
    "    palette=label_colors\n",
    ")\n",
    "\n",
    "# Draw arrows between centroids (use arrow patches)\n",
    "for src_label in transition_probs.index:\n",
    "    for tgt_label in transition_probs.columns:\n",
    "        prob = transition_probs.loc[src_label, tgt_label]\n",
    "        if prob > 0.05:  # threshold to avoid clutter\n",
    "            src = centroids_2d.loc[src_label]\n",
    "            tgt = centroids_2d.loc[tgt_label]\n",
    "\n",
    "            arrow = FancyArrowPatch(\n",
    "                (src[\"x\"], src[\"y\"]),\n",
    "                (tgt[\"x\"], tgt[\"y\"]),\n",
    "                arrowstyle='-|>',\n",
    "                mutation_scale=40,  # arrow size\n",
    "                linewidth=5,\n",
    "                color='green',\n",
    "                alpha=0.2 + prob/2  # use prob as transparency\n",
    "            )\n",
    "            ax.add_patch(arrow)\n",
    "\n",
    "# Annotate centroids\n",
    "for label, coord in centroids_2d.iterrows():\n",
    "    ax.text(coord[\"x\"], coord[\"y\"], label, fontsize=10, weight=\"bold\", ha='center', va='center')\n",
    "\n",
    "ax.set_title(\"Sample embedding + label transition graph\")\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.legend(loc=\"best\", title=\"Label\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = transition_probs.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directed graph\n",
    "G = nx.from_numpy_array(A, create_using=nx.DiGraph)\n",
    "\n",
    "# Extract weights for setting edge widths\n",
    "weights = [A[i][j] for i, j in G.edges()]\n",
    "\n",
    "# Optional: scale widths for better visibility\n",
    "max_width = 5\n",
    "min_width = 0.5\n",
    "normalized_weights = [\n",
    "    min_width + (w / max(weights)) * (max_width - min_width)\n",
    "    for w in weights\n",
    "]\n",
    "\n",
    "# Draw graph\n",
    "pos = nx.spring_layout(G)  # or try nx.circular_layout(G)\n",
    "nx.draw(\n",
    "    G, pos, with_labels=True, node_color=transition_probs.columns.map(label_colors),\n",
    "    arrows=True, width=normalized_weights,\n",
    "    edge_color='gray'\n",
    ")\n",
    "\n",
    "plt.title(\"Weighted Directed Graph (Edge Width = Weight)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydiffmap import diffusion_map as dm\n",
    "\n",
    "mydmap = dm.DiffusionMap.from_sklearn(n_evecs=3, alpha=0.5, epsilon='bgh')\n",
    "X_dm = mydmap.fit_transform(X)\n",
    "\n",
    "# X_dm[:, 0] is diffusion pseudotime-like coordinate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dm[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Build graph\n",
    "G = nx.Graph()\n",
    "for i, neighbors in enumerate(indices[:, 1:]):\n",
    "    for j in neighbors:\n",
    "        G.add_edge(i, j)\n",
    "\n",
    "# Get graph-based layout (like aligning samples by the transition graph)\n",
    "pos = nx.spring_layout(G, seed=42)  # dict: {sample_index: (x, y)}\n",
    "\n",
    "# Create embedding aligned to the graph\n",
    "graph_coords = np.array([pos[i] for i in range(len(X))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_color_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# Create graph\n",
    "G = nx.Graph()\n",
    "for i, neighbors in enumerate(indices[:, 1:]):  # skip self\n",
    "    for j in neighbors:\n",
    "        G.add_edge(i, j)\n",
    "\n",
    "# Get layout based on graph structure\n",
    "pos = nx.spring_layout(G, seed=137, k=0.01)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw_networkx_nodes(G, pos, \n",
    "                       node_color=label_color_values, \n",
    "                       node_size=40, alpha=0.8)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)\n",
    "plt.title(\"KNN Graph of Samples\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "threshold = 0.2\n",
    "\n",
    "# Get column clusters by cutting at this height\n",
    "col_clusters = fcluster(g.dendrogram_col.linkage, t=threshold, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "nmi = normalized_mutual_info_score(labels, col_clusters)\n",
    "print(f\"Normalized Mutual Information: {nmi:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Get the linkage matrix used for column clustering\n",
    "linkage = g.dendrogram_col.linkage\n",
    "\n",
    "# Set a number of clusters you want, e.g., 4\n",
    "num_clusters = 4\n",
    "\n",
    "# Extract flat cluster assignments\n",
    "col_clusters = fcluster(linkage, num_clusters, criterion='maxclust')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shape, non-negativity, and column sums\n",
    "assert D.shape == (n_features, 3), f\"Expected D shape {(n_features, 3)}, got {D.shape}\"\n",
    "assert Lambda.shape == (3, n_samples), f\"Expected Lambda shape {(3, n_samples)}, got {Lambda.shape}\"\n",
    "assert np.all(D >= 0), \"D contains negative values\"\n",
    "assert np.all(Lambda >= 0), \"Lambda contains negative values\"\n",
    "\n",
    "d_col_sums = D.sum(axis=0)\n",
    "lambda_col_sums = Lambda.sum(axis=0)\n",
    "np.testing.assert_allclose(d_col_sums, 1.0, atol=1e-4, err_msg=\"D columns do not sum to 1\")\n",
    "np.testing.assert_allclose(lambda_col_sums, 1.0, atol=1e-4, err_msg=\"Lambda columns do not sum to 1\")\n",
    "\n",
    "print(\"Julia notebook analog steps completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
